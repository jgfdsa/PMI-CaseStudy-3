{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales data to target variables\n",
    "## Author: Gregorio Ferreira - ferreiradesajg@gmail.com\n",
    "\n",
    "### Document description:\n",
    "This notebook, contains the steps followed to pre-process and extract the target variables to train the models considering the sales history of each of the stores under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The store codes that are repeated wihtihn the original dataset is/are:\n",
      "\n",
      "[11028]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import trim_mean\n",
    "import os\n",
    "os.chdir('XXX - YOUR PATH HERE - XXX')\n",
    "\n",
    "mydata = pd.read_csv(\"./data/sales_granular.csv\",\n",
    "                     index_col=0)\n",
    "\n",
    "## looking for duplicates\n",
    "test_dup = mydata[mydata.index.duplicated(keep=False)]\n",
    "\n",
    "print('The store codes that are repeated wihtihn the original dataset is/are:\\n')\n",
    "print(mydata.index.get_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of sales data:\n",
      "\n",
      "(903, 11936)\n"
     ]
    }
   ],
   "source": [
    "## Dropping the duplicates and reshaping the data into a tidy data frame:\n",
    "### Each feature represents store\n",
    "### Each row is an observation of the number of sales.\n",
    "\n",
    "mydata = mydata.drop_duplicates(keep='first') ## keep='first' just for verbose \n",
    "\n",
    "mydata_t = mydata.T\n",
    "\n",
    "mydata_t.index = pd.to_datetime(mydata_t.index.values,\n",
    "        format='%m/%d/%y %H:%M', errors='raise')  \n",
    "\n",
    "print('The shape of sales data:\\n')\n",
    "print(mydata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of assumptions \n",
    "- The analysis need to be done in less than 15h.\n",
    "- The size/type of the stores doesnt influence in the customer decision.\n",
    "- The inventory levels/capacity are elastic. The distribution/availability of the product is 100% ideal. (There as many products available as necessary)\n",
    "- The amount of revenue generated is the same for all the stores.\n",
    "  - If you travel less, you can give discounts to specific customers...\n",
    "- Since the amount of sales is an absolute quantity:\n",
    "  - The total quantity is independent of the type of product.\n",
    "  - All the products, including competitors products, have the same visibility/positioning in all the stores\n",
    "  - The quantity is independent from the prices of all the products across all the stores and from the competitors prices\n",
    "  - Other brands positioning/marketing-campaigns does not affect the sales of the product\n",
    "  - The price fluctuation, if any, does not affect the consumption of the product.\n",
    "- The surroundings and consumption\n",
    "  - The surroundings haven't changed during the period of time of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Resampling per months as the sum of the daily records.\n",
    "\n",
    "df_monthly = mydata_t.resample('M').sum()\n",
    "\n",
    "cols = list(df_monthly.columns)[1:]\n",
    "stores_sales = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of target variables\n",
    "### Continous targets:\n",
    "- 10% Trimmed mean\n",
    "- 20% Trimmed mean\n",
    "- Sales average\n",
    "- Minimum sales\n",
    "- Maximum sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Continuous target variables\n",
    "for col in cols:\n",
    "    stores_sales.append([col,\n",
    "                    df_monthly[col].dropna().count(),\n",
    "                    trim_mean(df_monthly[col].dropna(), 0.1),\n",
    "                    trim_mean(df_monthly[col].dropna(), 0.2),\n",
    "                    df_monthly[col].mean(),\n",
    "                    df_monthly[col].min(),\n",
    "                    df_monthly[col].max()])\n",
    "    \n",
    "labels = ['store_code', 'count_non_na', 'trim_mean_01',\n",
    "          'trim_mean_02', 'mean', 'min', 'max']\n",
    "\n",
    "stores_sales = pd.DataFrame(stores_sales, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrete targets:\n",
    "The discretization is done as the quantiles of prob 0,2/0,4/0,6/0,8 to create five categories bottom/low/standard/high/top from each of the continuous targets. These are part of the assumptions to clusters the stores into five buckets.\n",
    "\n",
    "- 10% Trimmed mean_class\n",
    "- 20% Trimmed mean_class\n",
    "- Sales average_class\n",
    "- Minimum sales_class\n",
    "- Maximum sales_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating quantiles classes\n",
    "cols = list(stores_sales.columns)[1:]\n",
    "col = cols[1]\n",
    "for col in cols:\n",
    "    \n",
    "    stores_sales[col+'_class'] = np.nan\n",
    "    \n",
    "    myq02 = stores_sales[col].quantile(0.2)\n",
    "    stores_sales.loc[stores_sales[col] < myq02, col+'_class'] = 'bottom'\n",
    "    \n",
    "    myq04 = stores_sales[col].quantile(0.4)\n",
    "    stores_sales.loc[(stores_sales[col] >= myq02) &\n",
    "                     (stores_sales[col] < myq04), col+'_class'] = 'low'\n",
    "    \n",
    "    myq06 = stores_sales[col].quantile(0.6)\n",
    "    stores_sales.loc[(stores_sales[col] >= myq04) &\n",
    "                     (stores_sales[col] < myq06), col+'_class'] = 'standard'\n",
    "    \n",
    "    myq08 = stores_sales[col].quantile(0.8)\n",
    "    stores_sales.loc[(stores_sales[col] >= myq06) &\n",
    "                     (stores_sales[col] < myq08), col+'_class'] = 'high'\n",
    "    \n",
    "    stores_sales.loc[stores_sales[col] >= myq08, col+'_class'] = 'top'\n",
    "\n",
    "\n",
    "stores_sales.to_csv(\"./result dataset/stores_sales.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
